**The source code for tagging Mead’s collection using Google Cloud Vision API, with adaptive filtering upon the Vision-generated labels.**

We tagged Mead’s collection of 21,996 web images twice. This is the description of the second round of tagging:

Two distinct Vision API requests, label_detection, and object_localization, were sent simultaneously to Vision for each image. The former request is for assigning labels, and the latter is for object recognition and localization. For each API request for a particular image, by default, ten tag terms with the highest confidence scores are returned. We decided to tag the collection with all Vision tags returned in the API responses by finding a match in the Python dictionary loaded from “polished_diction.pkl”. Subsequently, we filtered labels and objects depending on their confidence scores. In the first round, we observed that using a fixed constant threshold for filtering the Vision-generated tags seemed ineffective, so we considered the highest confidence score (HCS) returned in each label and object API response and included any matching Vision term in the range of [HCS - 0.15, HCS] for each API response.
Tagging the whole database would have taken nearly nine hours, so we divided the images into six batches to multi-process all six Python scripts concurrently. This resulted in a significantly less amount of tagging, and the whole collection was tagged in approximately half the time! Lastly, all six outputted CSV files were merged into one final file named “1.csv” in the “final file” subfolder
